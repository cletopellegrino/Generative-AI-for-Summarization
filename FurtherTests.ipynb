{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6114fa1f-34d2-48d6-b242-9fa415b94c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from bert_score import score\n",
    "\n",
    "train_df = pd.read_csv(\"dataset/train_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71865b10-ff3c-4cc7-8dad-8180195d863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "\n",
    "device = torch.device(\"cuda\") # GPU usage\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-large\")\n",
    "model_t5.to(device)\n",
    "\n",
    "tokenizer_t5.model_max_length = 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9585bbe-bfdb-4fd0-857d-cc6cf2efcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadingWithJumpingWindow(model, tokenizer, text):\n",
    "    torch.cuda.empty_cache()\n",
    "    if len(text.split()) < tokenizer.model_max_length:\n",
    "        inputs = tokenizer(\"summarize: \" + text,\\\n",
    "                           return_tensors=\"pt\",\\\n",
    "                           max_length=tokenizer.model_max_length,\\\n",
    "                           truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs, min_length=0, max_new_tokens=tokenizer.model_max_length,\\\n",
    "                                num_beams=4, early_stopping=True)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    JUMP = 100\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(words):\n",
    "        end = min(int(start + tokenizer.model_max_length + JUMP), len(words))\n",
    "        chunks.append(' '.join(words[int(start):int(start + tokenizer.model_max_length/2)]) \\\n",
    "                        + \" \" + ' '.join(words[int(start + tokenizer.model_max_length/2) + JUMP:end]))\n",
    "        start += tokenizer.model_max_length/2\n",
    "\n",
    "    print(len(chunks))\n",
    "    \n",
    "    summarized_chunks = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\"summarize: \" + chunk,\\\n",
    "                           return_tensors=\"pt\",\\\n",
    "                           max_length=tokenizer.model_max_length,\\\n",
    "                           truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs, min_length=0, max_new_tokens=tokenizer.model_max_length,\\\n",
    "                                num_beams=4, early_stopping=True)\n",
    "        summarized_chunks.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    summarized_text = ' '.join(summarized_chunks)\n",
    "    \n",
    "    if len(summarized_text.split()) > tokenizer.model_max_length:\n",
    "        return ReadingWithJumpingWindow(model, tokenizer, summarized_text)\n",
    "    else:\n",
    "        inputs = tokenizer(\"summarize: \" + summarized_text,\\\n",
    "                           return_tensors=\"pt\",\\\n",
    "                           max_length=tokenizer.model_max_length,\\\n",
    "                           truncation=True).to(device)\n",
    "        outputs = model.generate(**inputs, min_length=0, max_new_tokens=tokenizer.model_max_length,\\\n",
    "                                num_beams=4, early_stopping=True)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8fa2d-9298-4d56-af1f-e665a46cf5b4",
   "metadata": {},
   "source": [
    "The following test was done to assess the performance on the model on texts that it can analyze without splitting it in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392ebc7a-68fe-40b1-8544-c029aa7e6a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654\n",
      "equipment using ultra-wideband technology is defined as 'equipment incorporating a technology for short-range radiocommunication' decision is to harmonise the technical condition for the availability and efficient use of radio spectrum in the union.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.80\n",
      "3116\n",
      "this agreement shall apply to the civil aviation regulatory system of the people's republic of china and the civil aviation regulatory system of the european union. each party shall ensure that the other party is kept informed of all it relevant law, regulation, standard and procedure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.82\n",
      "92\n",
      "this regulation shall enter into force on the day following that of it publication in the official journal of the european union. it shall apply not later than 30 month after it entry into force.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.77\n",
      "0.7944852709770203\n"
     ]
    }
   ],
   "source": [
    "F1_t5_less_4096 = []\n",
    "\n",
    "for index, row in train_df[train_df['reference_tokens'] < 4096][:3].iterrows():\n",
    "    reference_text = row[\"reference\"]\n",
    "    reference_summary = row[\"summary\"]\n",
    "    print(row[\"reference_tokens\"])\n",
    "\n",
    "    result_summary = ReadingWithJumpingWindow(model_t5, tokenizer_t5, reference_text)\n",
    "    print(result_summary)\n",
    "    P, R, F1 = score([result_summary], [reference_summary], lang='en', verbose=False)\n",
    "    print(f\"T5 BertScore F1: {F1.item():.2f}\")\n",
    "    F1_t5_less_4096.append(F1.item())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "np.save('F1_t5_less_4096.npy', F1_t5_less_4096)\n",
    "\n",
    "sum = 0\n",
    "for _ in F1_t5_less_4096:\n",
    "    sum += _\n",
    "print(sum / len(F1_t5_less_4096))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766a8ea-8d1c-4e62-94ee-f2e994ae883b",
   "metadata": {},
   "source": [
    "The following test was done to assess the performance on the model on texts that it must split in chunks to perform the summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d5da47-d120-4303-ad26-d1ca8f9cda05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8443\n",
      "5\n",
      "eu has adopted a set of rules for the preparation of prospectuses. they include minimum information to be included in the registration document. additional information with respect to an entity other than the issuer shall be included in the security note.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.80\n",
      "19979\n",
      "10\n",
      "national administrator shall open an operator holding account in the union registry. account holder may request the removal of an authorised representative. national administrator may refuse to approve an authorised representative if the information and document provided are incomplete, out-of-date or false.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.80\n",
      "5394\n",
      "3\n",
      "up to 40 % of the total amount of the innovation fund support to a specific project shall be disbursed upon reaching the pre-determined milestone. the amount paid or to be paid to the project proponent in accordance with article 5 of the financial regulation shall be proportionately recovered or reduced.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.79\n",
      "0.7953208883603414\n"
     ]
    }
   ],
   "source": [
    "F1_t5_more_4096 = []\n",
    "\n",
    "for index, row in train_df[train_df['reference_tokens'] > 4096][:3].iterrows():\n",
    "    reference_text = row[\"reference\"]\n",
    "    reference_summary = row[\"summary\"]\n",
    "    print(row[\"reference_tokens\"])\n",
    "\n",
    "    result_summary = ReadingWithJumpingWindow(model_t5, tokenizer_t5, reference_text)\n",
    "    print(result_summary)\n",
    "    P, R, F1 = score([result_summary], [reference_summary], lang='en', verbose=False)\n",
    "    print(f\"T5 BertScore F1: {F1.item():.2f}\")\n",
    "    F1_t5_more_4096.append(F1.item())\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "np.save('F1_t5_more_4096.npy', F1_t5_more_4096)\n",
    "\n",
    "sum = 0\n",
    "for _ in F1_t5_more_4096:\n",
    "    sum += _\n",
    "print(sum / len(F1_t5_more_4096))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfcd31bc-8726-4a16-af1f-508f8767d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cleto\\AppData\\Local\\Temp\\ipykernel_13388\\3576928132.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "C:\\Python312\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "from datasets import load_metric, Dataset\n",
    "metric = load_metric(\"rouge\")\n",
    "from bert_score import score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df670fa0-5b5f-4fd7-9bfa-1a691785b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "MIN_ALLOWED_SEQUENCE = 30\n",
    "MAX_ALLOWED_SEQUENCE = 1024\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 3\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7070c8-e946-47db-b482-a2a8df4df825",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_length = MAX_ALLOWED_SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce603b8-4db7-4888-82ac-c457633fd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    references = [\"summarize: \" + ref for ref in examples[\"reference\"]]\n",
    "    \n",
    "    inputs = tokenizer(references, truncation=True, max_length=MAX_ALLOWED_SEQUENCE)\n",
    "    targets = tokenizer(examples[\"summary\"], truncation=True, max_length=MAX_ALLOWED_SEQUENCE)\n",
    "\n",
    "    # Update examples with tokenized inputs and targets\n",
    "    return {\"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask, \"labels\": targets.input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "195afa4e-5fef-468e-bbd6-4b7f867d58b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4923cd543a1e4a3783c6880e3d90fef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb325ec888994137a60f9db96bd3d5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "valid_df = pd.read_csv(\"validation_processed.csv\")\n",
    "\n",
    "train_df = train_df[train_df['reference_tokens'] < MAX_ALLOWED_SEQUENCE].reset_index(drop=True)\n",
    "valid_df = valid_df[valid_df['reference_tokens'] < MAX_ALLOWED_SEQUENCE].reset_index(drop=True)\n",
    "\n",
    "train_df[\"summary\"] = train_df[\"summary\"].apply(preprocess_text)\n",
    "valid_df[\"summary\"] = valid_df[\"summary\"].apply(preprocess_text)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2969c5e5-a885-4dd7-82ff-1a12a226f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21184b30-1c24-454e-b52d-f16f7d039ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./my_fine_tuned_t5_base_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    gradient_accumulation_steps=ACCUMULATION_STEPS,\n",
    "    eval_accumulation_steps=ACCUMULATION_STEPS,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f1c57b-9907-4b66-9498-07cb23106df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 1:36:10, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.143132</td>\n",
       "      <td>14.365900</td>\n",
       "      <td>6.707600</td>\n",
       "      <td>10.143900</td>\n",
       "      <td>13.778700</td>\n",
       "      <td>64.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.494305</td>\n",
       "      <td>18.553500</td>\n",
       "      <td>7.267200</td>\n",
       "      <td>12.881900</td>\n",
       "      <td>17.442500</td>\n",
       "      <td>225.142900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.405851</td>\n",
       "      <td>20.351800</td>\n",
       "      <td>8.014900</td>\n",
       "      <td>13.768900</td>\n",
       "      <td>19.072000</td>\n",
       "      <td>236.571400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1256: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=162, training_loss=5.149669505931713, metrics={'train_runtime': 5789.698, 'train_samples_per_second': 0.113, 'train_steps_per_second': 0.028, 'total_flos': 502460432593920.0, 'train_loss': 5.149669505931713, 'epoch': 2.958904109589041})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412987bf-6bf8-4e3c-82a5-18ed132ba1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "test_df = test_df[test_df['reference_tokens'] < MAX_ALLOWED_SEQUENCE].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c49e12-272e-4793-b4a6-9dc13667fa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the strategic innovation agenda of the european institute of innovation and technology for the period from 2021 to 2027 ( sia 2021-2027) shall be implemented in accordance with regulation ( eu ) 2021/819. article 3 decision no 1312/2013/eu is repealed with effect from 1 january 2021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.82\n",
      "the commission shall conduct an impact assessment on the continued issuance of 1- and 2-cent coin. the ceiling may be raised to 2,0 % of the cumulated total net number of 2-euro coin put into circulation by all member state whose currency is the euro up to the beginning of the year preceding the year of issuance of the commemorative coin. the identity of the issuing member state shall be clearly and easily recognisable on the coin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.82\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_trained = AutoModelForSeq2SeqLM.from_pretrained(\"./my_fine_tuned_t5_base_model/checkpoint-162\").to(device)\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    inputs = tokenizer(\"summarize: \" + row[\"reference\"], max_length=MAX_ALLOWED_SEQUENCE, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_trained.generate(**inputs, min_length=MIN_ALLOWED_SEQUENCE, max_length=MAX_ALLOWED_SEQUENCE,\\\n",
    "                                     num_beams=NUM_BEAMS, early_stopping=True)\n",
    "    result_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(result_summary)\n",
    "\n",
    "    P, R, F1 = score([result_summary], [row[\"summary\"]], lang='en', verbose=False)\n",
    "    print(f\"T5 BertScore F1: {F1.item():.2f}\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c77c01-c5ee-4cd2-9c69-dd0f52927586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sia 2021-2027 shall be implemented in accordance with regulation ( eu ) 2021/819. decision no 1312/2013/eu is repealed with effect from 1 january 2021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.79\n",
      "circulation coin means euro coin intended for circulation. commemorative coin means circulation coin intended to commemorate a specific subject. collector coin means euro coin intended for collection that are not issued with a view to their entry into circulation. member state may issue two commemorative coin per year.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 BertScore F1: 0.81\n"
     ]
    }
   ],
   "source": [
    "del model_trained\n",
    "device = torch.device(\"cuda\")\n",
    "model_untrained = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\").to(device)\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    inputs = tokenizer(\"summarize: \" + row[\"reference\"], max_length=MAX_ALLOWED_SEQUENCE, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_untrained.generate(**inputs, min_length=MIN_ALLOWED_SEQUENCE, max_length=MAX_ALLOWED_SEQUENCE,\\\n",
    "                                       num_beams=NUM_BEAMS, early_stopping=True)\n",
    "    result_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(result_summary)\n",
    "\n",
    "    P, R, F1 = score([result_summary], [row[\"summary\"]], lang='en', verbose=False)\n",
    "    print(f\"T5 BertScore F1: {F1.item():.2f}\")\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
